{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luck058/kaggle-gold-price-analysis/blob/model-1/model_1_kaggle_gold_price_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDLUcOHSvVIj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW59txoh2R9e"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/luck058/kaggle-gold-price-analysis\n",
        "\n",
        "%cd kaggle-gold-price-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbaanKahCY-w"
      },
      "source": [
        "# Create X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assets = [\"gold\", \"sp500\", \"nasdaq\", \"silver\", \"oil\", \"platinum\", \"palladium\"]\n",
        "\n",
        "# y_col = \"gold close diff\"\n",
        "y_col = \"gold close\""
      ],
      "metadata": {
        "id": "fABRbsa05K5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1bsi9Hlvb1M"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('financial_regression_cleaned.csv')\n",
        "\n",
        "original_cols = []\n",
        "for asset in assets:\n",
        "    # Only care about \"asset\"\n",
        "    original_cols += [f'{asset} open', f'{asset} high', f'{asset} low', f'{asset} close', f'{asset} volume']\n",
        "\n",
        "df = df[original_cols].reset_index(drop=True)\n",
        "\n",
        "display(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWsGA2SuyeIm"
      },
      "outputs": [],
      "source": [
        "for asset in assets:\n",
        "    df[f\"{asset} close diff\"] = df[f\"{asset} close\"].diff()\n",
        "\n",
        "    other_core_cols += [f'{asset} high-low', f'{asset} close-open', f'{asset} close diff']\n",
        "    df[f'{asset} high-low'] = df[f'{asset} high'] - df[f'{asset} low']\n",
        "    df[f'{asset} close-open'] = df[f'{asset} close'] - df[f'{asset} open']\n",
        "\n",
        "    display(df.head())\n",
        "    print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nXXXN8JCY-y"
      },
      "outputs": [],
      "source": [
        "def create_lag(df, column, lookback, include_zero=True):\n",
        "    assert column in df.columns\n",
        "    if include_zero:\n",
        "        df[f'{column}-0'] = df[column]\n",
        "\n",
        "    for lag in range(1, lookback):\n",
        "        if np.log2(lag) % 1 == 0:\n",
        "            df[f'{column}-{lag}'] = df[column].shift(lag) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1hevELVCY-y"
      },
      "outputs": [],
      "source": [
        "create_lag(df, y_col, 20, include_zero=True)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max(df, column, lookback, name_append=None):\n",
        "    assert column in df.columns\n",
        "    df[f'{column} max{name_append}'] = df[column].rolling(lookback).max()"
      ],
      "metadata": {
        "id": "zMKQqdB8bEq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_min(df, column, lookback, name_append=None):\n",
        "    assert column in df.columns\n",
        "    df[f'{column} min{name_append}'] = df[column].rolling(lookback).min()"
      ],
      "metadata": {
        "id": "cBCkOJACeqEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_min_max(df, column, lookback, name_append=None):\n",
        "    assert column in df.columns\n",
        "    get_max(df, column, lookback, name_append=name_append)\n",
        "    get_min(df, column, lookback, name_append=name_append)"
      ],
      "metadata": {
        "id": "ET_R-f8NetYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in original_cols + other_core_cols + [f\"{y_col}-0\"]:\n",
        "    get_min_max(df, column, 5, \" short\")\n"
      ],
      "metadata": {
        "id": "t2Z9bjtrenI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in original_cols + other_core_cols + [f\"{y_col}-0\"]:\n",
        "    get_min_max(df, column, 20, \" long\")\n"
      ],
      "metadata": {
        "id": "9NX4C_WVi5bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean(df, column, lookback, name_append=None):\n",
        "    assert column in df.columns\n",
        "    df[f'{column} mean{name_append}'] = df[column].rolling(lookback).mean()"
      ],
      "metadata": {
        "id": "RfroNDMGj1jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in original_cols + other_core_cols + [f\"{y_col}-0\"]:\n",
        "    get_min_max(df, column, 5, \" short\")"
      ],
      "metadata": {
        "id": "huVuVdjBj8hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in original_cols + other_core_cols + [f\"{y_col}-0\"]:\n",
        "    get_min_max(df, column, 20, \" long\")"
      ],
      "metadata": {
        "id": "_bEqzM1aj5p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-A3sk8bCY-z"
      },
      "source": [
        "## Create y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEK-SkKlCY-0"
      },
      "outputs": [],
      "source": [
        "df[y_col] = df[y_col].shift(-1)\n",
        "df = df.dropna(axis=0).reset_index(drop=True)\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lk9nUuACY-0"
      },
      "outputs": [],
      "source": [
        "y = df[y_col]\n",
        "X = df.drop(y_col, axis=1)\n",
        "\n",
        "print(\"y:\")\n",
        "display(pd.Series(y).head())\n",
        "print(\"X:\")\n",
        "display(X.head())\n",
        "\n",
        "print(\"len(y):\")\n",
        "print(len(y))\n",
        "print(\"X.shape:\")\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "-iA828QICgIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES8gS9EoCY-1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
        "y_class_train = y_train > y_train.mean()\n",
        "y_class_test = y_test > y_train.mean()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_normalized = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "print(\"y_class_train.shape:\", y_class_train.shape)\n",
        "print(\"y_class_test.shape:\", y_class_test.shape)\n",
        "\n",
        "plt.hist(y_train, bins=20, label=\"Train set\")\n",
        "plt.hist(y_test, bins=10, label=\"Test set\")\n",
        "plt.title(\"Distribution of y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.bar([0, 1], [len(y_class_train) - y_class_train.sum(), y_class_train.sum()], label=\"Train set\")\n",
        "plt.bar([0, 1], [len(y_class_test) - y_class_test.sum(), y_class_test.sum()], label=\"Test set\")\n",
        "plt.xticks([0, 1], [\"False\", \"True\"])\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of y_class\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XLKXGG98gsgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictZero:\n",
        "    \"\"\"Model which just predicts y as 0 irrespective of X\"\"\"\n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.zeros(len(X))\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return r2_score(y, y_pred)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "hD5wd3xmKI--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictOne:\n",
        "    \"\"\"Model which just predicts y as 0 irrespective of X\"\"\"\n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.ones(len(X))\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return r2_score(y, y_pred)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "a2g4vraeTplY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictPrevious:\n",
        "    \"\"\"Model which just predicts y as 0 irrespective of X\"\"\"\n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X[f\"{y_col}-0\"]\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return r2_score(y, y_pred)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "tHywGcwsqIQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictMean:\n",
        "    \"\"\"Model which just predicts y as the mean of y in the training set irrespective of X\"\"\"\n",
        "    def fit(self, X, y):\n",
        "        self.mean_y = np.mean(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(len(X), self.mean_y)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return r2_score(y, y_pred)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "Ge8yc918fKdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install ISLP"
      ],
      "metadata": {
        "id": "Tc4hLTHplrDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from ISLP.bart import BART\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n"
      ],
      "metadata": {
        "id": "Ep1jw0-2lp1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, y_train, other_scoring=[], return_estimator=False):\n",
        "    cv_results = cross_validate(model, X_train, y_train, scoring=[\"r2\", \"neg_mean_squared_error\"]+other_scoring, cv=10, return_estimator=return_estimator)\n",
        "    print(f\"R2: {print_cv_results(cv_results['test_r2'])}\")\n",
        "    print(f\"MSE: {print_cv_results(cv_results['test_neg_mean_squared_error'] * (-1))}\")\n",
        "    for scoring in other_scoring:\n",
        "        print(f\"{scoring}: {print_cv_results(cv_results[f'test_{scoring}'])}\")\n",
        "    print()\n",
        "    return cv_results"
      ],
      "metadata": {
        "id": "9mex3652yzFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_cv_results(results, z=1.96):\n",
        "    \"\"\"Prints the mean with confidence intervals (z-score can be modified, default 95% confidence)\"\"\"\n",
        "    mean = np.mean(results)\n",
        "    std_err = np.std(results) / np.sqrt(len(results))\n",
        "    return f\"{round(mean, 2)} \" + u\"\\u00B1\" + f\"{round(z * std_err, 2)}\""
      ],
      "metadata": {
        "id": "wpfdTCLMRVIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select models to test"
      ],
      "metadata": {
        "id": "CdRGaYF_po-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_regression_models = True # @param {type:\"boolean\"}\n",
        "run_tree_based_models = False # @param {type:\"boolean\"}\n",
        "run_classification_models = False # @param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "_X2rjsJvpr9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trivial models"
      ],
      "metadata": {
        "id": "MCMo3RauoYq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model: PredictZero\")\n",
        "evaluate_model(PredictZero(), X_train, y_train)\n",
        "\n",
        "print(f\"Model: PredictMean\")\n",
        "evaluate_model(PredictMean(), X_train, y_train)\n",
        "\n",
        "print(f\"Model: PredictPrevious\")\n",
        "evaluate_model(PredictPrevious(), X_train, y_train)\n",
        "pass"
      ],
      "metadata": {
        "id": "pJltLnbqulWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression/ lasso/ ridge"
      ],
      "metadata": {
        "id": "RxLlvfWHoB0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_regression_models:\n",
        "    print(f\"Model: LinearRegression\")\n",
        "    linear_regression = evaluate_model(LinearRegression(), X_train, y_train, return_estimator=True)\n",
        "    display(pd.DataFrame(zip(X.columns, linear_regression[\"estimator\"][0].coef_.round(3))).sort_values(by=1))"
      ],
      "metadata": {
        "id": "56Gzg8x5unBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_regression_models:\n",
        "    print(f\"Model: LinearRegression\")\n",
        "    linear_regression = evaluate_model(LinearRegression(), X_train_normalized, y_train, return_estimator=True)\n",
        "    display(pd.DataFrame(zip(X.columns, linear_regression[\"estimator\"][0].coef_.round(3))).sort_values(by=1))"
      ],
      "metadata": {
        "id": "7wEi9s4IdgLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_regression_models:\n",
        "    print(f\"Model: Ridge\")\n",
        "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
        "    best_error = -100\n",
        "    best_alpha = None\n",
        "    best_estimator = None\n",
        "    for alpha in alphas:\n",
        "        print(f\"alpha: {alpha}\")\n",
        "        ridge_results = evaluate_model(Ridge(alpha=alpha), X_train_normalized, y_train, return_estimator=True)\n",
        "        if ridge_results[\"test_neg_mean_squared_error\"].mean() > best_error:\n",
        "            best_error = ridge_results[\"test_neg_mean_squared_error\"].mean()\n",
        "            best_alpha = alpha\n",
        "            best_estimator = ridge_results[\"estimator\"][0]\n",
        "\n",
        "    print(\"best_alpha:\", best_alpha)\n",
        "    print(\"Best MSE:\", -best_error.round(2))\n",
        "    display(pd.DataFrame(zip(X.columns, best_estimator.coef_.round(3))).sort_values(by=1))\n"
      ],
      "metadata": {
        "id": "uBeh7lDduohN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_regression_models:\n",
        "    print(f\"Model: Lasso\")\n",
        "    alphas = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
        "    best_error = -100\n",
        "    best_alpha = None\n",
        "    best_estimator = None\n",
        "    for alpha in alphas:\n",
        "        print(f\"alpha: {alpha}\")\n",
        "        lasso_results = evaluate_model(Lasso(alpha=alpha, max_iter=100000), X_train_normalized, y_train, return_estimator=True)\n",
        "        if lasso_results[\"test_neg_mean_squared_error\"].mean() > best_error:\n",
        "            best_error = lasso_results[\"test_neg_mean_squared_error\"].mean()\n",
        "            best_alpha = alpha\n",
        "            best_estimator = lasso_results[\"estimator\"][0]\n",
        "\n",
        "    print(\"best_alpha:\", best_alpha)\n",
        "    print(\"Best MSE:\", -best_error.round(2))\n",
        "    display(pd.DataFrame(zip(X.columns, best_estimator.coef_.round(3))).sort_values(by=1))\n"
      ],
      "metadata": {
        "id": "C9U6wUrnupLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tree-based models"
      ],
      "metadata": {
        "id": "YuttWITcoHpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if run_tree_based_models:\n",
        "#     print(f\"Model: BART\")\n",
        "#     evaluate_model(BART(), X_train, y_train)\n",
        "#     pass"
      ],
      "metadata": {
        "id": "JTYpdwl3uu5a",
        "outputId": "da624f1a-7405-4147-e083-6f47023c5f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BART\n",
            "R2: 0.99 ±0.0\n",
            "MSE: 7.16 ±0.77\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tree_based_models:\n",
        "    print(f\"Model: RandomForestRegressor\")\n",
        "    random_forest = RandomForestRegressor()\n",
        "    grid_search = GridSearchCV(random_forest, param_grid={'n_estimators': [30, 100, 300]}, cv=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "    print(\"best_n_estimators:\", best_n_estimators)\n",
        "    evaluate_model(RandomForestRegressor(n_estimators=best_n_estimators), X_train, y_train)\n",
        "    pass"
      ],
      "metadata": {
        "id": "wml1x9sjfQjW",
        "outputId": "2c6d2581-5158-4539-a73e-02b3d66c70f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: RandomForestRegressor\n",
            "best_n_estimators: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_tree_based_models:\n",
        "    print(f\"Model: RegressionTree with Pruning\")\n",
        "    regressor = DecisionTreeRegressor()\n",
        "    ccp_path = regressor.cost_complexity_pruning_path(X_train, y_train)\n",
        "    ccp_path.alphas = ccp_path.ccp_alphas[::len(ccp_path.ccp_alphas)//3]\n",
        "    grid_search = GridSearchCV(regressor, param_grid={'ccp_alpha': ccp_path.ccp_alphas}, cv=3)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_ccp_alpha = grid_search.best_params_['ccp_alpha']\n",
        "    print(\"best_ccp_alpha:\", best_ccp_alpha)\n",
        "    evaluate_model(DecisionTreeRegressor(ccp_alpha=best_ccp_alpha), X_train, y_train)\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "sNB6yba4NGUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Model: MLPRegressor\")\n",
        "# evaluate_model(MLPRegressor(max_iter=2000, hidden_layer_sizes=(100,100,100)), X_train, y_train)"
      ],
      "metadata": {
        "id": "SMEvQnzao8My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification models"
      ],
      "metadata": {
        "id": "NcW_Jdi6oQaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_classification_models:\n",
        "    print(f\"Model: PredictZero\")\n",
        "    evaluate_model(PredictZero(), X_train, y_class_train, other_scoring=[\"accuracy\"])\n",
        "\n",
        "    print(f\"Model: PredictOne\")\n",
        "    evaluate_model(PredictOne(), X_train, y_class_train, other_scoring=[\"accuracy\"])\n",
        "\n",
        "    print(f\"Model: LogisticRegression\")\n",
        "    evaluate_model(LogisticRegression(), X_train, y_class_train, other_scoring=[\"accuracy\"])\n",
        "\n",
        "    print(f\"Model: LinearDiscriminantAnalysis\")\n",
        "    evaluate_model(LogisticRegression(), X_train, y_class_train, other_scoring=[\"accuracy\"])\n",
        "\n",
        "    print(f\"Model: QuadraticDiscriminantAnalysis\")\n",
        "    evaluate_model(LogisticRegression(), X_train, y_class_train, other_scoring=[\"accuracy\"])\n",
        "\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "id": "4chdsadrRZiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('financial_regression_cleaned.csv')\n",
        "\n",
        "# Only care about \"asset\"\n",
        "original_cols = [f'{asset} open', f'{asset} high', f'{asset} low', f'{asset} close', f'{asset} volume']\n",
        "df2 = df2[original_cols].reset_index(drop=True)\n",
        "df2[f'{asset} high-low'] = df2[f'{asset} high'] - df2[f'{asset} low']\n",
        "df2[f'{asset} close-open'] = df2[f'{asset} close'] - df2[f'{asset} open']\n",
        "\n",
        "display(df2.head())\n",
        "print(df2.shape)"
      ],
      "metadata": {
        "id": "am8Z7H3OCnqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df[original_cols+other_core_cols]"
      ],
      "metadata": {
        "id": "DzhaILzTDDT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df2[20:25])"
      ],
      "metadata": {
        "id": "u9cglGfeKudf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df3.head())\n",
        "print(df3.shape)"
      ],
      "metadata": {
        "id": "UTnTp2bDjccX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use best model"
      ],
      "metadata": {
        "id": "SkECgN1EMQSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = LinearRegression()\n",
        "# model_1 = PredictPrevious()\n",
        "model_1 = Lasso(alpha=0.001, max_iter=100000)"
      ],
      "metadata": {
        "id": "heJNNHk1NFeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.fit(X_train, y_train)\n",
        "predictions = model_1.predict(X_test)\n",
        "actual = y_test\n",
        "print(\"R2:\", round(r2_score(actual, predictions), 3))\n",
        "print(\"MSE:\", round(mean_squared_error(actual, predictions), 3))\n"
      ],
      "metadata": {
        "id": "MyhZYMThMOUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwNDNE3oNlno"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}